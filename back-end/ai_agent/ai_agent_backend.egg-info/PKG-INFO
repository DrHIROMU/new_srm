Metadata-Version: 2.4
Name: ai-agent-backend
Version: 0.1.0
Summary: AI agent backend that proxies OpenAI chat completions and enterprise data APIs.
Author: SRM Team
License: Proprietary
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: fastapi>=0.115.0
Requires-Dist: uvicorn[standard]>=0.30.0
Requires-Dist: httpx>=0.27.0
Requires-Dist: openai>=1.51.0
Requires-Dist: pydantic-settings>=2.3.3
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: chromadb>=0.4.0
Requires-Dist: sentence-transformers>=2.2.0
Provides-Extra: dev
Requires-Dist: pytest>=8.3.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.23.7; extra == "dev"
Requires-Dist: httpx[http2]>=0.27.0; extra == "dev"
Requires-Dist: anyio>=4.4.0; extra == "dev"
Requires-Dist: faker>=26.0.0; extra == "dev"

# AI Agent Backend

Python FastAPI service that brokers conversations between the SRM front-end, OpenAI, and internal APIs. It exposes a lightweight REST surface that the UI can call to initiate LLM chats while requesting supplemental data from enterprise systems.

## Prerequisites

- Python 3.11 or later
- OpenAI API key with access to the desired chat models
- Access credentials for the enterprise APIs the agent will query

## Project Structure

- `app/main.py` - FastAPI application factory
- `app/api/routes.py` - HTTP endpoints (`/api/chat`, `/api/health`)
- `app/models/` - Pydantic models shared by the application
- `app/services/` - Domain services for OpenAI and enterprise APIs
- `app/clients/openai_client.py` - Thin wrapper over the OpenAI SDK
- `tests/` - Pytest + FastAPI TestClient smoke coverage
- `.env.example` - Template for required environment variables
- `pyproject.toml` - Project metadata and dependencies

## Getting Started

1. Create a virtual environment and activate it.
2. Install dependencies with `pip install -e .[dev]` from this directory.
3. Copy `.env.example` to `.env` and populate the secrets:
   - `OPENAI_API_KEY`, `OPENAI_MODEL`, `OPENAI_TIMEOUT_SECONDS`
   - `EXTERNAL_API_BASE_URL`, `EXTERNAL_API_KEY`, `EXTERNAL_API_TIMEOUT_SECONDS`
   - `CORS_ALLOW_ORIGINS` (comma-separated list of allowed front-end origins, e.g. `http://localhost:4200`)
   - (Optional) enable knowledge search with `KNOWLEDGE_SEARCH_ENABLED=true` and configure `CHROMA_*` values
4. Launch the API using `uvicorn app.main:app --reload --host 0.0.0.0 --port 8081`.

The service will expose OpenAPI docs at `http://localhost:8081/api/docs`.

## Knowledge base integration (optional)

The backend can enrich prompts with passages stored in [ChromaDB](https://www.trychroma.com/). To enable it:

1. Run (or connect to) a Chroma server, e.g. `docker run -p 8000:8000 chromadb/chroma`.
2. Set the following variables in `.env`:
   ```ini
   KNOWLEDGE_SEARCH_ENABLED=true
   CHROMA_HOST=127.0.0.1
   CHROMA_PORT=8000
   CHROMA_COLLECTION=srm-knowledge-base
   CHROMA_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
   CHROMA_TOP_K=4
   ```
3. Ingest documents with the helper script:
   ```bash
   python dev_tools/chroma_ingest.py ./docs \
     --chroma-host 127.0.0.1 --chroma-port 8000 \
     --collection srm-knowledge-base
   ```
   The script accepts `.txt` and `.md` files by default and will chunk them with sentence-transformer embeddings.

Whenever a user sends a prompt, the latest user message is used as a semantic search query; the top matches are injected into the system prompt so the agent can ground responses on your curated corpus.

## Testing

```
pytest
```

Tests use dependency overrides to avoid real OpenAI or enterprise API calls.

## Next Steps

- Wire `DataService` (`app/services/data_service.py`) to the concrete endpoints that will be provided.
- Decide on authentication strategy for the front-end (`Authorization` headers, session tokens, etc.).
- Extend `LLMService` if tool-calling or function-calling workflows are needed.
- Add deployment automation once the integration paths are stable (Dockerfile, CI stages, etc.).
